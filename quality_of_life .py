# -*- coding: utf-8 -*-
"""Quality of Life.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFINxus-YzY6F8zNoZeUSEQqQZjvX7AF
"""

from google.colab import drive
drive.mount('/content/drive')

"""# Importing the Data"""

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_squared_log_error
import shap
import numpy as np
from sklearn.decomposition import PCA
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
ql = pd.read_csv("/content/drive/My Drive/Business Analytics/Self Project/Quality of Life Index/Quality_of_Life.csv")
ql

"""# Data Preprocessing"""

ql.info()

ql['Quality of Life Value'] = ql['Quality of Life Value'].str.replace('[:\']', '', regex=True).astype(float)
# Removing '' from data in categorical column
categorical_columns = ['Purchasing Power Category', 'Safety Category', 'Health Care Category',
                       'Climate Category', 'Cost of Living Category', 'Property Price to Income Category',
                       'Traffic Commute Time Category', 'Pollution Category', 'Quality of Life Category']
for col in categorical_columns:
    ql[col] = ql[col].str.replace("'", "", regex=True)
ql

ql['Property Price to Income Value'] = pd.to_numeric(ql['Property Price to Income Value'], errors='coerce')
ql['Quality of Life Value'] = pd.to_numeric(ql['Quality of Life Value'], errors='coerce')

ql.info()

ql.head()

"""## Data Cleaning"""

ql.isnull().sum()

"""Using KNN Imputer to handling missing value in numerical and categorical variable"""

from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Select the numerical and categorical columns to be imputed
numeric_columns = ['Purchasing Power Value', 'Safety Value', 'Health Care Value',
                   'Climate Value', 'Cost of Living Value', 'Property Price to Income Value',
                   'Traffic Commute Time Value', 'Pollution Value', 'Quality of Life Value']

# Scaling the numerical data
scaler = StandardScaler()
ql[numeric_columns] = scaler.fit_transform(ql[numeric_columns])

# Using KNN Imputer to replace missing values ​​in numeric columns
imputer_numeric = KNNImputer(n_neighbors=5)  # Menggunakan 5 tetangga terdekat untuk imputasi
ql[numeric_columns] = imputer_numeric.fit_transform(ql[numeric_columns])

# Impute missing values ​​in a categorical column with mode (most frequently occurring value)
imputer_categorical = SimpleImputer(strategy='most_frequent')
ql[categorical_columns] = imputer_categorical.fit_transform(ql[categorical_columns])

# After imputation, we return the numerical data to their original scale
ql[numeric_columns] = scaler.inverse_transform(ql[numeric_columns])

# Checking the imputation results
ql

"""## Feature Engineering"""

category_mapping = {'Very Low': 1, 'Low': 2, 'Moderate': 3, 'High': 4, 'Very High': 5, None: 0}
columns_to_map = ['Purchasing Power Category', 'Safety Category', 'Health Care Category',
                  'Climate Category', 'Cost of Living Category', 'Property Price to Income Category',
                  'Traffic Commute Time Category', 'Pollution Category', 'Quality of Life Category']

for col in columns_to_map:
    ql[col] = ql[col].map(category_mapping)
ql

from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()

ql['country'] = label_encoder.fit_transform(ql['country'])
ql

# Adding additional features

ql['Income_to_Property_Ratio'] = ql['Purchasing Power Value'] / (ql['Property Price to Income Value'] + 1e-6)
ql['Safety_to_Pollution_Ratio'] = ql['Safety Value'] / (ql['Pollution Value'] + 1e-6)
ql['Health_Index_Score'] = (ql['Health Care Value'] + ql['Climate Value']) / 2

ql.head()

"""## Scaling Numerical Data"""

scaler = StandardScaler()
ql[numeric_columns] = scaler.fit_transform(ql[numeric_columns])
ql

"""# EDA (Exploratory Data Analysis)

### Data Distribution
"""

ql[numeric_columns].hist(figsize=(12, 8), bins=20)
plt.show()

"""### Correlation"""

plt.figure(figsize=(10, 8))
sns.heatmap(ql[numeric_columns].corr(), annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Matrix")
plt.show()

"""# Model Training and Machine Learning"""

# Using Quality of Life as a target for regression
X = ql.drop(columns=['Quality of Life Value', 'Quality of Life Category'])
y = ql['Quality of Life Value']

# PCA for Dimensionality Reduction
pca = PCA(n_components=5)
X_pca = pca.fit_transform(X)

# Split the Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Evaluation Function
def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    y_pred = model.predict(X_test)
    rmse = mean_squared_error(y_test, y_pred) ** 0.5
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    cv_r2 = cross_val_score(model, X_train, y_train, cv=5, scoring='r2').mean()

    print(f"{model_name} Performance:")
    print(f"RMSE: {rmse:.4f}, MAE: {mae:.4f}, R²: {r2:.4f}, CV R²: {cv_r2:.4f}\n")

    return y_pred

# 1️⃣ Linear Regression
lr = LinearRegression()
lr.fit(X_train, y_train)
y_pred_lr = evaluate_model(lr, X_train, X_test, y_train, y_test, "Linear Regression")

# 2️⃣ Random Forest Regressor with GridSearchCV
rf_params = {
    'n_estimators': [100, 200],
    'max_depth': [None, 20, 30],
    'min_samples_split': [2, 5]
}
rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid=rf_params, cv=3, scoring='r2')
rf.fit(X_train, y_train)
y_pred_rf = evaluate_model(rf.best_estimator_, X_train, X_test, y_train, y_test, "Random Forest")

# 3️⃣ XGBoost Regressor with SHAP Feature Importance
xgb = XGBRegressor(objective="reg:squarederror", random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = evaluate_model(xgb, X_train, X_test, y_train, y_test, "XGBoost")

# SHAP Analysis
explainer = shap.Explainer(xgb)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

"""## Clustering using K-Means with PCA-Reduced Data"""

from sklearn.cluster import KMeans
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, random_state=42, n_init=10)
    kmeans.fit(X_pca)
    wcss.append(kmeans.inertia_)

plt.plot(range(1, 11), wcss, marker='o')
plt.xlabel('Total Cluster')
plt.ylabel('WCSS')
plt.title('Elbow Method for Determining K')
plt.show()

kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
ql['Cluster'] = kmeans.fit_predict(X_pca)

sns.scatterplot(x=ql['Purchasing Power Value'], y=ql['Quality of Life Value'], hue=ql['Cluster'])
plt.title("Clustering K-Means Result")
plt.show()

"""# Hyperparameter Tuning"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
import shap

# Hyperparameter Grid for Randomized Search
param_dist = {
    "n_estimators": [50, 100, 200, 300],
    "max_depth": [None, 10, 20, 30, 50],
    "min_samples_split": [2, 5, 10],
    "min_samples_leaf": [1, 2, 5, 10],
    "max_features": ["sqrt", "log2", None],
    "bootstrap": [True, False]
}

# Stage 1: Randomized Search
rf_random = RandomizedSearchCV(
    RandomForestRegressor(random_state=42),
    param_distributions=param_dist,
    n_iter=20, cv=5, scoring="r2", n_jobs=-1, random_state=42
)

rf_random.fit(X_train, y_train)
best_params = rf_random.best_params_
print("Best Parameters from RandomizedSearchCV:", best_params)

# Stage 2: Grid Search with Best Parameters from Randomized Search
param_grid = {
    "n_estimators": [best_params["n_estimators"] - 50, best_params["n_estimators"], best_params["n_estimators"] + 50],
    "max_depth": [None, best_params["max_depth"], best_params["max_depth"] + 10 if best_params["max_depth"] is not None else None],
    "min_samples_split": [best_params["min_samples_split"]],
    "min_samples_leaf": [best_params["min_samples_leaf"]],
    "max_features": [best_params["max_features"]],
    "bootstrap": [best_params["bootstrap"]]
}

rf_grid = GridSearchCV(
    RandomForestRegressor(random_state=42),
    param_grid=param_grid,
    cv=5, scoring="r2", n_jobs=-1
)

rf_grid.fit(X_train, y_train)
best_rf = rf_grid.best_estimator_

# Evaluate Tuned Model
y_pred_rf_tuned = best_rf.predict(X_test)
evaluate_model(best_rf, X_train, X_test, y_train, y_test, "Tuned Random Forest")

# Feature Importance with SHAP
explainer = shap.Explainer(best_rf)
shap_values = explainer(X_test)

shap.summary_plot(shap_values, X_test)

